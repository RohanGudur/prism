{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "368991da-73f0-41e4-9db6-f4af354e4ce0"
      },
      "source": [
        "# &#x1F916; THEME 5 SOLUTION BY TEAM RAGAPIS &#x1F320;"
      ],
      "id": "368991da-73f0-41e4-9db6-f4af354e4ce0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aead955-0231-4ead-a730-8fb95447fe81"
      },
      "source": [
        "<b>Team Name</b> : RAGAPIS <br>\n",
        "<b>Team ID </b>  : nil<br>\n",
        "<b>Hack Name</b> : vectorRAG<br>\n",
        "<b>Theme</b>     : 5<br>\n",
        "<b>Members</b>   : Rohan Gudur, Rohit Sinha<br>\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Name</th>\n",
        "    <th>Email</th>\n",
        "    <th>Branch & Year</th>\n",
        "    <th>Phone</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Rohan G<!-- Member 1 Details ---></td>\n",
        "    <td>rohan.g2022@vitstudent.ac.in</td>\n",
        "    <td>ECE, 2nd year</td>\n",
        "    <td>91104 22054</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>Rohit Sinha<!-- Member 2 Details ---></td>\n",
        "    <td>rohit.sinha2022@vitstudent.ac.in</td>\n",
        "    <td>CSE,2nd year</td>\n",
        "    <td>93539 58508</td>\n",
        "  </tr>\n",
        "</table>"
      ],
      "id": "1aead955-0231-4ead-a730-8fb95447fe81"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e6b0c8b-bd10-4bcd-9019-64fd48408bdb"
      },
      "source": [
        "<div>\n",
        "<div style=\"display:flex \">\n",
        "    <div style=\"margin: auto auto auto 0\"><h2>Motivation  &#x2b50;</h2>  </div>\n",
        "    <div style=\" margin: auto 0 auto auto \"><b>[20 Points]</b></div>\n",
        "</div>\n",
        "<i>A compelling motivation sets the stage for an unforgettable hack.\n",
        "What inspired you to take this up? What real-world problem does it solves? Tell us your story!\n",
        "\n",
        "----------------------------------------------\n",
        "</i>\n",
        "<i>\n",
        "<h3>Background/Motivation:</h3>\n",
        "\n",
        "In a booming economy like ours where everyone is busy contributing value and growth in their own way, keeping in  check with a varying range of information from various sources such as to-do lists, calendars, meeting notes, and bills. The increasing reliance on digital tools for managing these aspects of our lives necessitates a more interactive and efficient way to engage with this data. The current challenge lies in the lack of seamless interaction with these disparate sources to derive meaningful insights and suggestions proactively.\n",
        "\n",
        "Speaking from personal experience we would often find ourselves overwhelmed from the requirement to keep a track of all our tasks at hand and this lead inspired us to work on this theme.\n",
        "<h3>Solution:</h3>\n",
        "\n",
        "To address this need, we propose ** a working RAG LLM pipeline solution that responds back as an assistant named bixby**. This system integrates with the user's to-do list, calendar, meeting information, notes, bills, and other relevant data sources. By leveraging generative AI technology, our vision is to empower users with proactive prompts and suggestions. This includes facilitating interactive Q&A, summarizing important information, and extracting actionable items from the user's data. The ultimate goal is to enhance the user's ability to interact effectively with their personal data, fostering a more streamlined and productive digital experience.\n",
        "\n",
        "</i>\n",
        "</div>"
      ],
      "id": "4e6b0c8b-bd10-4bcd-9019-64fd48408bdb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "456372ba-7d13-44ed-a4a8-faa3a2550bbf"
      },
      "source": [
        "<div>\n",
        "<div style=\"display:flex \">\n",
        "    <div style=\"margin: auto auto auto 0\"><h2>Implementation  &#x1f6e0;</h2>  </div>\n",
        "    <div style=\" margin: auto 0 auto auto\"><b>[70 Points]</b></div>\n",
        "</div>\n",
        "<i>Breakdown your nitty-gritty project's architechture,features, technologies down here. Provide us with enough details to grasp your technical prowess, but make sure it is snappy and engaging. Flowcharts, Snippets, diagram are your allies here\n",
        "\n",
        "\n",
        "</i>\n",
        "</div>"
      ],
      "id": "456372ba-7d13-44ed-a4a8-faa3a2550bbf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuMDM_qiN48X"
      },
      "source": [
        "\n",
        "  <ul>\n",
        "    <li><h3>Calander And To-Do Data Extraction Via API</h3></li>\n",
        "  </ul>"
      ],
      "id": "AuMDM_qiN48X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVksnmZNpxT2"
      },
      "source": [
        "**install necessary packages**"
      ],
      "id": "hVksnmZNpxT2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1T2PzsCRhpZ",
        "outputId": "fcaab2da-2eb2-444d-9650-7bf0e7155a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.17.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth) (0.5.1)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (2.17.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-auth-oauthlib) (4.9)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.31.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-auth-oauthlib) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib) (2024.2.2)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from google-auth-httplib2) (2.17.3)\n",
            "Requirement already satisfied: httplib2>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-httplib2) (0.22.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-auth-httplib2) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-auth-httplib2) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-auth-httplib2) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->google-auth-httplib2) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->google-auth-httplib2) (0.5.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.84.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (1.62.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client) (3.1.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2024.2.2)\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.31.0)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2024.2.2)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install google-auth\n",
        "!pip install google-auth-oauthlib\n",
        "!pip install google-auth-httplib2\n",
        "!pip install google-api-python-client\n",
        "!pip install gTTS\n",
        "#for this implementation we use the widely used google calendar this can be further replaced with others services based on user preferences"
      ],
      "id": "J1T2PzsCRhpZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S3M63uTpsTb"
      },
      "source": [
        "**import necessary packages**"
      ],
      "id": "3S3M63uTpsTb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCHzbKycQWUd"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary modules from the Google Colab and Google API libraries\n",
        "from google.colab import auth  # Importing the authentication module from the Google Colab library\n",
        "from google.auth.transport.requests import Request  # Importing the Request module from the Google Auth Transport Requests library\n",
        "from google.oauth2.credentials import Credentials  # Importing the Credentials module from the Google OAuth2 library\n",
        "from googleapiclient.discovery import build  # Importing the build module from the Google API client library for creating service objects\n",
        "from googleapiclient.errors import HttpError  # Importing the HttpError module from the Google API client library for handling HTTP errors\n",
        "\n",
        "# Importing the datetime and os modules from the Python standard library\n",
        "import datetime  # Importing the datetime module for working with dates and times\n",
        "import os  # Importing the os module for interacting with the operating system\n",
        "\n",
        "from google.colab import drive #google drive integration\n",
        "from google.colab import files #google files integration"
      ],
      "id": "eCHzbKycQWUd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOci8LUZpovZ"
      },
      "source": [
        "**mount drive onto project(to store the temporary output file)**"
      ],
      "id": "vOci8LUZpovZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6-pDa7yQi_X",
        "outputId": "596b64aa-15e2-4c75-e810-daaae7cb4f20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/grdive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/grdive') # Mount google drive"
      ],
      "id": "u6-pDa7yQi_X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqehNSLYEBs7"
      },
      "source": [
        "**Defining SCOPES variable and write function**"
      ],
      "id": "vqehNSLYEBs7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgUEZddcEBIu"
      },
      "outputs": [],
      "source": [
        "# Define a constant variable SCOPES, which is a list containing a single string\n",
        "# representing the scope required for accessing Google Calendar data in read-only mode.\n",
        "SCOPES = [\"https://www.googleapis.com/auth/calendar.readonly\"]\n",
        "\n",
        "# Define a function named write_file that takes 'data' as its parameter.\n",
        "def write_file(data):\n",
        "    # Open a file named 'events.txt' in append mode ('a+'), creating it if it doesn't exist,\n",
        "    # and assigning it to the file variable. 'a+' mode allows both reading and appending to the file.\n",
        "    with open('events.txt','a+') as file:\n",
        "        # Write the provided 'data' to the file opened earlier.\n",
        "        # The 'data' parameter is appended to the end of the file.\n",
        "        file.write(data)\n"
      ],
      "id": "rgUEZddcEBIu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TDMYSAtFOcK"
      },
      "source": [
        "**Upload the credentials.json from the google API config dashboard**\n",
        "\n"
      ],
      "id": "9TDMYSAtFOcK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsLT40UeTpvG"
      },
      "source": [
        "<h4>*Note - Setup the the credentials and token(This is only temporary as the the o2Auth is not possible in colab if you are running this on your system skip this upload and only get your credentials.json this will be made into a seamless process in future implementations)</h4>\n",
        "<ul>\n",
        "  <li>Goto google API</li>\n",
        "  <li>Create a new project</li>\n",
        "  <li>Enable APIs</li>\n",
        "  <li>Configure OAuth</li>\n",
        "  <li>Create clientID</li>\n",
        "  <li>Configure application type</li>\n",
        "  <li>Create clientID</li>\n",
        "  <li>Download Credentials.json</li>\n",
        "  <li>Now export this code as a py file</li>\n",
        "  <li>Run the code it will and complete the Oauth</li>\n",
        "  <li>It well generate a token.json</li>\n",
        "  <li>Then run the code block below</li>\n",
        "\n",
        "</ul>"
      ],
      "id": "FsLT40UeTpvG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "pTD5hFiKFCbU",
        "outputId": "e0c9cf02-9d10-4ed7-e9c6-9bbdd89113aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your credential.json and token.json file:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e7041261-65e7-439b-a447-e215167a1521\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e7041261-65e7-439b-a447-e215167a1521\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Print a message prompting the user to upload their credential.json and token.json files\n",
        "print(\"Please upload your credential.json and token.json file:\")\n",
        "\n",
        "# Allow the user to upload their credential.json file and store it in the 'uploaded' variable\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Allow the user to upload their token.json file and store it in the 'uploadedToken' variable\n",
        "uploadedToken = files.upload()\n",
        "\n",
        "# Iterate over each uploaded file in the 'uploaded' dictionary and print a success message for each file\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Uploaded {filename} successfully.\")\n",
        "\n",
        "# Iterate over each uploaded file in the 'uploadedToken' dictionary and print a success message for each file\n",
        "for filename in uploadedToken.keys():\n",
        "    print(f\"Uploaded {filename} successfully.\")\n"
      ],
      "id": "pTD5hFiKFCbU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v26iIq42pPOF"
      },
      "source": [
        "**Read schedule data using google API**"
      ],
      "id": "v26iIq42pPOF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeLDEW22UahS",
        "outputId": "c84e247a-d436-47b9-84dc-84b8d9ae3776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting the upcoming 10 events\n",
            "2024-02-11T03:00:00+05:30 Event 1\n",
            "2024-02-11T06:30:00+05:30 Event 2\n",
            "2024-02-11T09:30:00+05:30 Event 3\n"
          ]
        }
      ],
      "source": [
        "# Function to fetch schedule data from Google Calendar\n",
        "def fetchScheduleData():\n",
        "    creds = None\n",
        "\n",
        "    # Check if token.json file exists to load credentials\n",
        "    if os.path.exists(\"token.json\"):\n",
        "        creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
        "\n",
        "    # If credentials not valid or not found, attempt to refresh or create new credentials\n",
        "    if not creds or not creds.valid:\n",
        "        if creds and creds.expired and creds.refresh_token:\n",
        "            creds.refresh(Request())\n",
        "        else:\n",
        "            # If no valid credentials found, initiate OAuth flow to obtain new credentials\n",
        "            flow = InstalledAppFlow.from_client_secrets_file(\n",
        "                \"credentials.json\", SCOPES\n",
        "            )\n",
        "            creds = flow.run_console()\n",
        "\n",
        "        # Save the obtained credentials to token.json file\n",
        "        with open(\"token.json\", \"w\") as token:\n",
        "            token.write(creds.to_json())\n",
        "\n",
        "    try:\n",
        "        # Build the Calendar API service using obtained credentials\n",
        "        service = build(\"calendar\", \"v3\", credentials=creds)\n",
        "\n",
        "        # Get the current time in UTC format\n",
        "        now = datetime.datetime.utcnow().isoformat() + \"Z\"\n",
        "\n",
        "        # Print message indicating fetching upcoming events\n",
        "        print(\"Getting the upcoming 10 events\")\n",
        "\n",
        "        # Fetch upcoming events from the primary calendar\n",
        "        events_result = (\n",
        "            service.events()\n",
        "            .list(\n",
        "                calendarId=\"primary\",\n",
        "                timeMin=now,\n",
        "                maxResults=10,\n",
        "                singleEvents=True,\n",
        "                orderBy=\"startTime\",\n",
        "            )\n",
        "            .execute()\n",
        "        )\n",
        "\n",
        "        # Get the list of events\n",
        "        events = events_result.get(\"items\", [])\n",
        "\n",
        "        # If no events found, print a message and return\n",
        "        if not events:\n",
        "            print(\"No upcoming events found.\")\n",
        "            return\n",
        "\n",
        "        # Iterate through each event and print its start time and summary\n",
        "        for event in events:\n",
        "            start = event[\"start\"].get(\"dateTime\", event[\"start\"].get(\"date\"))\n",
        "            print(start, event[\"summary\"])\n",
        "\n",
        "            # Concatenate start time and summary, then write to file\n",
        "            filestream = start + event['summary'] + \"\\n\"\n",
        "            write_file(filestream)\n",
        "\n",
        "    except HttpError as error:\n",
        "        # If an HTTP error occurs, print the error message\n",
        "        print(f\"An error occurred: {error}\")\n",
        "\n",
        "# Call the function to fetch schedule data\n",
        "fetchScheduleData()"
      ],
      "id": "MeLDEW22UahS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3DmIheepEjY"
      },
      "source": [
        "**Move the event details file into the drive for analysis**"
      ],
      "id": "x3DmIheepEjY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPr16lOePy2N"
      },
      "outputs": [],
      "source": [
        "!cp /content/events.txt /content/grdive/MyDrive"
      ],
      "id": "MPr16lOePy2N"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efnuOHb1p30q"
      },
      "source": [
        "\n",
        "  <ul>\n",
        "    <li><h3>RAG Gen AI implementation</h3></li>\n",
        "  </ul>"
      ],
      "id": "efnuOHb1p30q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqoPf4ZHqB1t"
      },
      "source": [
        "**Installing python packages**"
      ],
      "id": "yqoPf4ZHqB1t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca744757",
        "outputId": "1dfbf36d-6a6e-456e-d13a-9ee5769ada32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain PyPDF2 llama-index pinecone-client openai python-dotenv tiktoken gradio faiss-cpu  pydantic==1.10.11\n",
        "!pip install -U sentence-transformers"
      ],
      "id": "ca744757"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QwbpUcBpW3B"
      },
      "source": [
        "## Setting Up the Environment"
      ],
      "id": "8QwbpUcBpW3B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY6KYNUnSPs-"
      },
      "outputs": [],
      "source": [
        "# Loads environment variables from a .env file\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "# Enables reading and parsing PDF files\n",
        "from PyPDF2 import PdfReader\n",
        "# Splits text into characters for language processing tasks\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "# Connects to Pinecone, a vector database for efficient similarity search\n",
        "from pinecone import Pinecone, PodSpec\n",
        "# Provides a FAISS-based vector store for storing and querying text embeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "# Loads a pre-trained question-answering pipeline from LangChain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "# Uses OpenAI's LLMs for language processing tasks\n",
        "from langchain.llms import OpenAI\n",
        "# Manages OpenAI API usage and rate limits\n",
        "from langchain.callbacks import get_openai_callback\n",
        "# Interacts with the operating system for file system operations\n",
        "import os\n",
        "# Provides a modern way to work with file paths\n",
        "from pathlib import Path\n",
        "#  library for counting the number of tokens in a text string without making an API call. when working with OpenAI models.\n",
        "import tiktoken\n",
        "# Creates sentence embeddings for comparing text similarity\n",
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "id": "cY6KYNUnSPs-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x3-q0F84d9W"
      },
      "source": [
        "<h4>*Note - API Configuration(Follow these steps before running the next cell)</h4>\n",
        "<ul>\n",
        "  <li>Click on the key symbol in the left menu</li>\n",
        "  <li>Click on add a new secret</li>\n",
        "  <li>Type 'OPENAI_API_KEY' in the \"name\" section and the openai API key in the \"value\" section</li>\n",
        "  <li>Click on add new secret again</li>\n",
        "  <li>Then type 'PINECONE_API' in the \"name\" section and the pinecone API in the \"value\" section</li>\n",
        "</ul>"
      ],
      "id": "5x3-q0F84d9W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gFllrCnEL2q"
      },
      "outputs": [],
      "source": [
        "## Setting up API-Keys and Credentials for OpenAI and Pinecone:\n",
        "#following is for collab\n",
        "from google.colab import userdata\n",
        "\n",
        "# OpenAI API_Key.\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Pinecone API_Key and IndexName.\n",
        "api_key = userdata.get('PINECONE_API')\n",
        "index_name = \"prism\"\n",
        "PINECONE_API_ENV=\"gcp-starter\"\n",
        "\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n"
      ],
      "id": "1gFllrCnEL2q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "eh8PQNW-TwsF",
        "outputId": "342c336a-2d7c-4a01-fa52-a94fa748a491"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name sentence-transformers/all-MiniLM-L6-v2. Creating a new one with MEAN pooling.\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like sentence-transformers/all-MiniLM-L6-v2 is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 503 Server Error: Service Temporarily Unavailable for url: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1239\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 503 Server Error: Service Temporarily Unavailable for url: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1370\u001b[0m             \u001b[0;31m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             raise LocalEntryNotFoundError(\n\u001b[0m\u001b[1;32m   1372\u001b[0m                 \u001b[0;34m\"An error happened while trying to locate the file on the Hub and we cannot find the requested files\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-3e4e74be168a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#downloading an embedding model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membedding_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, trust_remote_code, revision, token, use_auth_token)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 )\n\u001b[1;32m    201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 modules = self._load_auto_model(\n\u001b[0m\u001b[1;32m    203\u001b[0m                     \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m_load_auto_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code)\u001b[0m\n\u001b[1;32m    965\u001b[0m             )\n\u001b[1;32m    966\u001b[0m         )\n\u001b[0;32m--> 967\u001b[0;31m         transformer_model = Transformer(\n\u001b[0m\u001b[1;32m    968\u001b[0m             \u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    678\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_missing_entries\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_connection_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0;34mf\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this file, couldn't find it in the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;34mf\" cached files and it looks like {path_or_repo_id} is not the path to a directory containing a file named\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like sentence-transformers/all-MiniLM-L6-v2 is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
          ]
        }
      ],
      "source": [
        "#downloading an embedding model\n",
        "embed_model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "embedding_dims = embed_model.encode(['']).shape[1]"
      ],
      "id": "eh8PQNW-TwsF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQKXHU2EUZVv"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# Function 1 - Extracts text from the '.pdf' or '.txt' file provided.\n",
        "def extract_text_from_file(file_path):\n",
        "    try:\n",
        "        _, file_extension = os.path.splitext(file_path.lower())\n",
        "\n",
        "        if file_extension == '.pdf':\n",
        "            with open(file_path, 'rb') as pdf_file:\n",
        "                pdf_reader = PdfReader(pdf_file)\n",
        "                text = ''\n",
        "                num_pages = len(pdf_reader.pages)\n",
        "\n",
        "                for page_num in range(num_pages):\n",
        "                    page = pdf_reader.pages[page_num]\n",
        "                    text += page.extract_text()\n",
        "\n",
        "                return text\n",
        "        elif file_extension == '.txt':\n",
        "            with open(file_path, 'r', encoding='utf-8') as txt_file:\n",
        "                text = txt_file.read()\n",
        "                return text\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Function 2 - Tokenizes and Splits the text extracted in the above function to make chunks of 2000 tokens each.\n",
        "def tokenize_and_split(inp, chunk_size=200):\n",
        "      text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_size//2)\n",
        "      return [i.page_content for i in text_splitter.create_documents(texts = [inp])]\n",
        "\n",
        "# Function 3 - Embed the text chunks\n",
        "def embed(chunks, model = embed_model):\n",
        "    return model.encode([chunk.replace(\"\\n\", \" \") for chunk in chunks])\n",
        "    # return [client.embeddings.create(input = [chunk.replace(\"\\n\", \" \")], model=model).data[0].embedding for chunk in chunks]\n",
        "\n",
        "# Main Function 1 - Pushes data from the document uploaded to the Pinecone Index using multiple smaller functions.\n",
        "def push_data_to_index(pdf_path):\n",
        "  extracted_text = extract_text_from_file(pdf_path)\n",
        "  text = tokenize_and_split(extracted_text)\n",
        "  output = embed(text)\n",
        "  vectors = upsert_chunks_to_pinecone(output, text, api_key, index_name)"
      ],
      "id": "FQKXHU2EUZVv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_Xej86_Ukfp"
      },
      "outputs": [],
      "source": [
        "# functions that deal with the pinecone database\n",
        "offset = 0\n",
        "# Function 3 - Upserts the chunks to your Pinecone Index.\n",
        "def upsert_chunks_to_pinecone(vectors, chunks, api_key, index_name, token_size=embedding_dims):\n",
        "    global offset\n",
        "    pc = Pinecone(api_key=api_key)\n",
        "    index = pc.Index(index_name)\n",
        "    # vectors_padded = [vector + [0.0] * (token_size - len(vector)) if len(vector) < token_size else vector for vector in vectors]\n",
        "    vectors_float = [[float(value) for value in vector] for vector in vectors]\n",
        "\n",
        "    upsert_data = [{\"id\": f\"{i+offset}\",\n",
        "                    \"values\": vectors_float[i],\n",
        "                    \"metadata\": {\"text\": chunks[i]}} for i in range(len(vectors_float))]\n",
        "    print\n",
        "    offset += len(vectors_float)\n",
        "    index.upsert(vectors=upsert_data)\n",
        "\n",
        "# Function 4 - Queries your Pinecone Index for chunks that match with the question.\n",
        "def query_pinecone(question, api_key, index_name, token_size=embedding_dims):\n",
        "    pc = Pinecone(api_key=api_key)\n",
        "    index = pc.Index(index_name)\n",
        "    question_vector = tokenize_and_split(question)\n",
        "    question_vector = embed(question_vector)\n",
        "\n",
        "    return index.query(\n",
        "      vector=question_vector[0].tolist(),\n",
        "      top_k=5,\n",
        "      include_metadata = True\n",
        "    )\n",
        "\n",
        "# Function 5 - Fetches the vectors for the matched chunks from the Pinecone Index.\n",
        "def fetch_from_pinecone(ids, api_key, index_name):\n",
        "    pc = Pinecone(api_key=api_key)\n",
        "    index = pc.Index(index_name)\n",
        "    return index.fetch(ids)"
      ],
      "id": "G_Xej86_Ukfp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W88G0uOeVYFr"
      },
      "outputs": [],
      "source": [
        "## Defining the Main Functions:\n",
        "\n",
        "\n",
        "# Main Function 2 - Queries the Pinecone Index for matches, Fetches the data from matches AND  Answers the users question using a couple of smaller functions.\n",
        "def start_QnA(question, instructions, encode = False):\n",
        "  prompt = '''{instructions}\n",
        "            CONTEXT : {data}'''\n",
        "  match_results = query_pinecone(question, api_key, index_name)\n",
        "  match_ids = [match['id'] for match in match_results['matches']]\n",
        "\n",
        "  fetch_results = fetch_from_pinecone(match_ids, api_key, index_name)\n",
        "  fetched_data = [vector['metadata']['text'] for vector in fetch_results['vectors'].values()]\n",
        "\n",
        "\n",
        "  final_context_after_fetching_and_decoding = '\\n'.join(fetched_data)\n",
        "\n",
        "  gpt_response_choices = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-0125\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": prompt.format(instructions=instructions, data=final_context_after_fetching_and_decoding),\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"{question}\"\"\",\n",
        "            },\n",
        "        ],\n",
        "        stream = True\n",
        "    )\n",
        "  partial_message = \"\"\n",
        "  for chunk in gpt_response_choices:\n",
        "      if chunk.choices[0].delta.content:\n",
        "          partial_message = partial_message + chunk.choices[0].delta.content\n",
        "          yield partial_message"
      ],
      "id": "W88G0uOeVYFr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw2KxmI1VZaj"
      },
      "outputs": [],
      "source": [
        "## DOCUMENT UPLOAD\n",
        "\n",
        "# acccesses the '.pdf' or '.txt' file to the colab environment generated by the users that we send as data to implement rag on\n",
        "pdf_path = 'output.txt'\n",
        "\n",
        "#print(type(pdf_path))\n",
        "push_data_to_index(pdf_path)"
      ],
      "id": "Jw2KxmI1VZaj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOY0H9ctWJmc"
      },
      "outputs": [],
      "source": [
        "#defining the role instructions\n",
        "role_instructions =\"\"\"You are a SAMSUNG VOICE  assistant bot named bixby and you have been given some textual information, your job is to answer the users questions only from the information provided in the\n",
        "                context and the user chat history.\n",
        "\n",
        "                IMPORTANT NOTES FOR THE TASK:\n",
        "                - Use ONLY the context provided to answer the question.\n",
        "                - DO NOT make up any information yourself to answer the question.\n",
        "                - If the answer is not found in the context provided, DO NOT answer the question with random data and reply stating that the context does not have information to answer the question.\"\"\""
      ],
      "id": "vOY0H9ctWJmc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAorI5PoWmdD"
      },
      "outputs": [],
      "source": [
        "## interacting with the assistant\n",
        "from gtts import gTTS\n",
        "from io import BytesIO\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Write your question below:\n",
        "     # we can also ask the question as audio if required\n",
        "question=input(\"Enter Question: \")\n",
        "\n",
        "# start_QnA returns a string response\n",
        "res = start_QnA(question, role_instructions)\n",
        "\n",
        "response_text = list(res)[-1]\n",
        "\n",
        "# Convert text to speech\n",
        "tts = gTTS(response_text, lang='en')\n",
        "\n",
        "# Save the audio to a BytesIO object\n",
        "audio_bytes = BytesIO()\n",
        "tts.write_to_fp(audio_bytes)\n",
        "audio_bytes.seek(0)\n",
        "\n",
        "# Play the audio\n",
        "ipd.Audio(audio_bytes.read(), autoplay=True)\n"
      ],
      "id": "NAorI5PoWmdD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mrpm5EC1kio"
      },
      "source": [
        "<div>\n",
        "    <div style=\"display:flex\">\n",
        "        <div style=\"margin:auto auto auto 0\">\n",
        "            <h2>Speculation &#x1f680;</h2>\n",
        "        </div>\n",
        "        <div style=\"margin:auto 0 auto auto\">\n",
        "            <b>[10 Points]</b>\n",
        "        </div>\n",
        "    \n",
        "  </div>\n",
        "  <h3>Potential Advancements in use case Technology: </h3>\n",
        "     <ul>\n",
        "        <li> As open source local llms and embedding models get more compute efficient and better, we can use ** LOCAL LLMS AND EMBEDDING MODELS**, making it safe and convenient when dealing with private and confidential information that must not leave your premises..,                                                                                                                           \n",
        "        We could also use FINE TUNED models along with RAG that are the specific use casses of working on meeting data or phone conversations</li>\n",
        "    \n",
        "   </ul>\n",
        "   <h3>Integration with Emerging Technologies :</h3>\n",
        "     <ul>\n",
        "        <li>Continued technological evolution may see integration with emerging technologies like augmented reality (AR) and [metaverse] virtual reality (VR) to enhance the immersive nature of virtual communication and transcription experiences. This could potentially enable new ways of visualizing and interacting with conversational data in virtual environments.</li>\n",
        "      </ul>\n",
        "  <h3>Considerations for Accessibility and Inclusivity:</h3>\n",
        "  <ul>\n",
        "        <li>Given the widespread use of virtual communication, there's heightened emphasis on accessibility and inclusivity in transcription tools. Future developments may prioritize improvements in accessibility features, such as support for different languages, dialects, and accessibility tools for users with diverse needs.</li>\n",
        " </ul>\n",
        "  <h3>Privacy and Security Considerations:</h3>\n",
        "  <ul>\n",
        "        <li>As conversations become increasingly digitized and transcribed, concerns about privacy and security are likely to escalate. Innovations in encryption, data anonymization, and consent management and how we implement the following in our end to end pipelines may address these concerns, ensuring that sensitive information remains protected.</li>\n",
        "  </ul>\n",
        "   <p>In summary, with the advent of more development in the field of generative AI, we can find solutions that run locally on one's device and are more inclusive while keeping it integrated into their augmented and virtual realities without the tradeoff of privacy. </p>\n",
        "</div>\n"
      ],
      "id": "1Mrpm5EC1kio"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}